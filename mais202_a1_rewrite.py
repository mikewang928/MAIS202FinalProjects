# -*- coding: utf-8 -*-
"""MAIS202 A1 rewrite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AVV_hN1i7Hif80UNuihtFYUYesRsqNyp
"""

!pip install numpy
!pip install pandas 
!pip install matplotlib

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

########################## DATA ORGANIZATION ##############################
data_train_url = 'https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_test.csv'
data_valid_url = 'https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_valid.csv'
data_test_url = 'https://raw.githubusercontent.com/cclin130/mais-202-assignment-2-f2019/master/Dataset_1_test.csv'

data_train = pd.read_csv(data_train_url, header=None).sort_values(0)
data_valid = pd.read_csv(data_valid_url, header=None).sort_values(0)
data_test = pd.read_csv(data_test_url, header=None).sort_values(0)

# Training data
#print(data_train)
X_train = np.array(data_train.iloc[:,0]) #colomn 0 of every row
X_train = np.reshape(X_train,(len(X_train),1)) 
Y_train = np.array(data_train.iloc[:,1])
Y_train = np.reshape(Y_train,(len(Y_train),1))
#print(X_train)

# Validation data
X_valid = np.array(data_valid.iloc[:,0])
X_valid = np.reshape(X_valid,(len(X_valid),1))
Y_valid = np.array(data_valid.iloc[:,1])
Y_valid = np.reshape(Y_valid,(len(Y_valid),1))


# Testing data
X_test = np.array(data_test.iloc[:,0])
X_test = np.reshape(X_test,(len(X_test),1))
Y_test = np.array(data_test.iloc[:,1])
Y_test = np.reshape(Y_test,(len(Y_test),1))

############### vistialazing the graph ################
plt1 = plt.scatter(X_train, Y_train, s=10) 
#plt2 = plt.scatter(X_valid, Y_valid, s=10)
#plt3 = plt.scatter(X_test, Y_test, s=10)

#############################################################################
# PART I: Linear regression with closed form ordinary least square solution #
# NOTE: least square solution (CS229 lecture 1): W=(X^T*X)^(-1)X^T*Y        #
#############################################################################
# STEP 1: create a matrix Ï•(X) for the train, valid, and test data
'''
np.hstack: Stack arrays in sequence horizontally (column wise)
np.append(axis=1): Append the data into the array column by column 
'''

X_train_poly = np.ones((50,1), dtype=float)
#print(X_train_poly)

X_matrix_train = np.empty((50,2),dtype=float)
ones = np.empty((50,0), dtype=float)
for i in range (1,len(X_train)+1):
  ones = np.append(1,ones)
  ones = np.reshape(ones, (len(ones),1))
#print(ones)
#print(len(X_train))
X_matrix_train = np.hstack((ones, X_train))
#print(X_matrix_train)
for i in range (1,16):
  X_train_poly = np.append(X_train_poly, np.power(X_train,i),axis=1)
#print(X_train_poly)
#print(X_train)



X_valid_poly = np.ones((50,1), dtype=float)
X_matrix_valid = np.ones((50,1), dtype=float)
X_matrix_valid = np.hstack((X_matrix_valid, X_valid))
for i in range(1,16):
  X_valid_poly = np.append(X_valid_poly, np.power(X_valid,i),axis=1)
#print(X_valid_poly)
#print(X_matrix_valid)



X_test_poly = np.ones((50,1), dtype=float)
X_matrix_test = np.ones((50,1),dtype=float)
X_matrix_test = np.hstack((X_matrix_test, X_test))
for i in range (1,16):
  X_test_poly = np.append(X_test_poly, np.power(X_test,i), axis=1)
 
# Step 2: checks that your X_poly's have the correct dimensions:
assert (X_train_poly.shape[0] == 50 and X_train_poly.shape[1] == 16)
assert (X_valid_poly.shape[0] == 50 and X_valid_poly.shape[1] == 16)
assert (X_test_poly.shape[0] == 50 and X_test_poly.shape[1] == 16)

# Step 3: Prepare the parameter of the "W" in the linear regression formula
############### least square solution: W=(X^T*X)^(-1)X^T*Y ################
W=np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X_train_poly),X_train_poly)),np.transpose(X_train_poly)),Y_train)
#print(W)
#print(W.shape)

# assert the shape of the W
assert (W.shape[0]==16 and W.shape[1]==1)

# Step 4: make prediction
######################            Training             #########################
### YOUR CODE HERE
# Make predictions on the training set
# make training set predictions, save to variable 'y_train_pred'
# Hint (for matrix multiplication errors):
# in the math above, the matrix X had dimensions
# (polynomial degree x num samples). However, your X_poly's have
# dimensions (num samples x polynomial degree).
y_train_pred = np.dot(X_train_poly, W)

# this checks that W has the correct dimensions:
assert (y_train_pred.shape[0] == 50 and y_train_pred.shape[1] == 1)

### YOUR CODE HERE 
# calculate mean squared error, save to variable 'mse_train':
mse_train = np.square(np.subtract(Y_train,y_train_pred)).mean()
print("Training set Mean Squared Error: {}".format(mse_train))



######################            Validation           #########################
# Make predictions on the validation set 
y_valid_pred = np.dot(X_valid_poly,W)
# Calculate the MSE and save to 'mse_valid'
mse_valid = np.square(np.subtract(Y_valid,y_valid_pred)).mean()
print("validation set Mean Square Error: {}".format(mse_valid))

# Step 5: plot the traning set with weight
'''
np.poly1d:https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html
np.flip():https://numpy.org/doc/stable/reference/generated/numpy.flip.html
'''
function = np.poly1d(np.flip(W[:,0],0),False) #flip along the row, poly1d start with a decend power order 
x_axis = np.linspace(min(X_train), max(X_train))#generate x-axis data label
plt.plot(x_axis,function(x_axis),'r',label='15 Degree Polynomial Fit')
plt.scatter(X_train, Y_train, s=10, label='Training set')
plt.xlabel("x")
plt.ylabel("f(x) for training set and prediction")
plt.title("Training set and model (closed form solution)")
plt.show()

#############################################################################################################################
# PART II: Linear regression with gradiant descent(CS229 lecture 1)                                                         #
# NOTE:                                                                                                                     #
#       Cost function:      J(W) = (sum{from i=0 to i=m}((transpose(W)*x(ith)-y(ith))^2)))/(2m)                             #
#       gradiant descent:   w(jth) = w(jth)-(alpha*sum{from i=0 to i=m}((transpose(W)*x(ith)-y(ith)*x(ith_jth)))/m          #
# where:                                                                                                                    #
#        w(jth)        the parameter of the jth W                                                                           #
#        x(ith_jth)    the jth parameter of vector X(ith)                                                                   #
#        ith           Sample number out of a total fo m samples                                                            #
#To implement the gradient descent algorithm, we will need:                                                                 #
#    1, a function that calculates the gradients of J (the cost function), with respect to each entry in W (i.e. each  wj ) #
#    2, a function that calculates the change in the values of W after each gradient descent update                         #
#    3, a while loop that performs gradient descent by caling the above 2 functions, until the change in W < epsilon        #
#############################################################################################################################

# We start by defining the relevant constants 
learning_rate = 0.55
epsilon = 0.0003
lambda_value = 0.3

# Weight matrix will be 16by1
# we initialize the weights at 0
W = np.zeros((16,1))

#############################################################################
# PART II: Linear regression with gradiant descent                          #
#############################################################################

'''
np.matmul: https://numpy.org/doc/1.18/reference/generated/numpy.matmul.html 
np.sum: https://numpy.org/doc/1.18/reference/generated/numpy.sum.html
enumerate():
'''

# Step 1: function that calculates the gradient
def calculate_grad(X_poly, y, W):
  dW = np.zeros((16,1))
  m = len(X_poly)
  y_pred = np.matmul(X_poly, W)
  for j, w_j in enumerate(W):
    #dW[j] = np.matmul((np.matmul(X_poly, W)-y),X_poly)/m
    dW[j] = np.matmul(np.transpose(y_pred - y), X_poly[:,j])/m
  return dW


# function that calculate the changes in W
def calculate_dist(W_prev, W_cur):
  return np.sqrt(np.sum((W_cur - W_prev)**2))      # make sure under squart there is a positive number  

test = np.sqrt(np.sum((5 - 1)**2))
print(test)  

# function to decrible grdient decent
def train_polynomial_regression(X_poly, y, W, learning_rate, epsilon):
  epoch_count = 0
  while True:
    dW = calculate_grad(X_poly, y, W)
    W_prev = W.copy()
    W = W_prev - learning_rate*dW
    diff = calculate_dist(W_prev, W)
    if (diff < epsilon):
      break
    
    epoch_count +=1
    if epoch_count % 20000 == 0: 
      y_train_pred = np.matmul(X_train_poly, W)
      print('Training set Mean Squared Error: {}'.format(np.power((y_train_pred - Y_train),2).mean()))

  print('Training complete.')
  return W

# Step 2: calculate the MSE of the model 
############################ trainning beginnnings  ##############################
W = train_polynomial_regression(X_train_poly, Y_train, W, learning_rate, epsilon)

################################## VALIDATION ####################################
#calculated MSE
'''
y_valid_pred = np.matmul(X_valid_poly, W)
mse_valid = np.power((y_valid_pred - Y_valid), 2).mean()
print('\nValidation set Mean Squared Error: {}'.format(mse_valid))

'''
y_valid_pred = np.matmul(X_valid_poly, W)
mse_valid = np.square(np.subtract(y_valid_pred, Y_valid)).mean()
#mse_vaild = np.square(np.subtract(y_valid_pred, Y_valid)).mean()
print('\nValidation set Mean Squared Error: {}'.format(mse_valid))

# step 3: plot the training set using weights from gradient descent
function = np.poly1d(np.flip(W[:, 0], 0)) 

x_axis = np.linspace(min(X_train), max(X_train))
plt.plot(x_axis, function(X_train), 'r', label = '15 Degree Polynomial Fit') 
plt.scatter(X_train, Y_train, s=10, label='Training set')
plt.xlabel("X")
plt.ylabel("f(x) for dataset and model")
plt.title("Training Set and Model (Gradient descent solution)")
plt.show()

# plot the validation set using weights from gradient descent
function = np.poly1d(np.flip(W[:, 0], 0)) 

x_axis = np.linspace(min(X_valid), max(X_valid))
plt.plot(x_axis, function(X_valid), 'r', label = '15 Degree Polynomial Fit') 
plt.scatter(X_valid, Y_valid, s=10, label='validation set')
plt.xlabel("X")
plt.ylabel("f(x) for dataset and model")
plt.title("Training Set and Model (Gradient descent solution)")
plt.show()

#############################################################################################################################
# PART III: Linear regression with gradiant descent and regularlization (CS229 lecture 7)                                   #
# where:                                                                                                                    #
#        w(jth)        the parameter of the jth W                                                                           #
#        x(ith_jth)    the jth parameter of vector X(ith)                                                                   #
#        ith           Sample number out of a total fo m samples                                                            #
#To implement the gradient descent algorithm, we will need:                                                                 #
#    1, a function that calculates the gradients of J (the cost function), with respect to each entry in W (i.e. each  wj ) #
#    2, a function that calculates the change in the values of W after each gradient descent update                         #
#    3, a while loop that performs gradient descent by caling the above 2 functions, until the change in W < epsilon        #
#############################################################################################################################

"""Recall that with regularization, the Residual Sum of Squares equation becomes:

$$ RSS(W) =  \sum_{i=0}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right)^2 + \lambda \cdot \sum_{j=1}^{p} w_j^2$$

Where _i_ represents the sample number out of a total of *m* samples and $w_j$ represents the jth parameter of W out of _p_ parameters. The reason j starts at 1 in the regularization sum is because we normally don't regularize the bias term $w_0$.

Making these same changes to our cost function from Q2, we have:

$$ J(W) = \frac{1}{2m}\left[\sum_{i=0}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right)^2 + \lambda \cdot \sum_{j=1}^{p} w_j^2\right]$$

Once again, the $\frac{1}{m}$ is to calculate the mean of the squared errors, and the $\frac{1}{2}$ is to make the gradient nicer.

Now, when we take the partial derivative of J(W) with respect to weight $w_j$, the jth parameter of vector _W_, we get a different result for $w_0$ than for the rest of the parameters:

$$ \begin{aligned} \frac{dJ}{dw_0} &= \frac{1}{m}\sum_{i=1}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right) \cdot x_j^{(i)} \\
 \frac{dJ}{dw_j} &= \frac{1}{m}\left(\sum_{i=1}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right) \cdot x_j^{(i)}\right) + \frac{\lambda}{m} \cdot w_j \quad \text{for}\ j = 1, 2, \ldots, p \end{aligned} $$

Putting this all together into the gradient descent algorithm for regularized linear regression gives us:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $w_j$ in W:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>if j = 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>$\displaystyle w_j := w_j - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right) \cdot x_j^{(i)}$<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>else<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<t>$\displaystyle w_j := w_j - \alpha \cdot \left(\left(\frac{1}{m}\sum_{i=1}^{m} \left(W\transpose x^{(i)} - y^{(i)}\right) \cdot x_j^{(i)}\right) + \frac{\lambda}{m} \cdot w_j\right) $<br>
    
We can run the gradient descent update for as many itertions as needed until the amount the gradients change each loop is negligible (less than a given _epsilon_).
"""

# Step 0ï¼šwe start by defining the relevant constants (the same as Q2)
learning_rate = 0.55
epsilon = 0.0003
lambda_value = 0.3

# weight matrix will be 16x1
# we initialize the weights at 0
W = np.zeros((16, 1))

# Step 1: function that calculates the gradient
def calculate_regularized_grad(X_poly, y, W, lambda_value):
  dW = np.zeros((16,1))
  m = len(X_poly)
  y_pred = np.matmul(X_poly, W)
  for j,w_j in enumerate(W):
    if j ==0 : 
      dW[j] = np.dot(np.transpose(y_pred - y), X_poly[:,j])/m 
    else: 
      dW[j] = np.dot(np.transpose(y_pred - y), X_poly[:,j])/m + lambda_value*w_j/m 
  return dW

# function that caculates the change in W
def calculate_dist(W_prev, W_cur):
    return np.sqrt(np.sum((W_cur - W_prev)**2))

# function of gradient decent 
def train_regularized_polynomial_regression(X_poly, y, W, learning_rate, epsilon, lambda_value, verbose=True):
  epoch_count = 0
  while True:
      dW = calculate_regularized_grad(X_poly, y, W, lambda_value)
      W_prev = W.copy()
      W = W_prev - learning_rate*dW
      diff = calculate_dist(W_prev, W)
      if (diff < epsilon):
          break
      epoch_count +=1
      # print train error every 50 iterations
      if verbose:
        if epoch_count % 100 == 0:
          y_train_pred = np.matmul(X_train_poly, W)
          print('Training set Mean Squared Error: {}'.format(np.power((y_train_pred - Y_train), 2).mean()))

  print('Training complete.')
  return W

# Step 2: calculate the MSE of the model 
############################ trainning beginnnings  ##############################
W = train_regularized_polynomial_regression(X_train_poly, Y_train, W, learning_rate, epsilon, lambda_value)

################################## VALIDATION ####################################
#calculated MSE
y_valid_pred = np.matmul(X_valid_poly, W)
mse_valid = np.square(np.subtract(y_valid_pred, Y_valid)).mean()
#mse_vaild = np.square(np.subtract(y_valid_pred, Y_valid)).mean()
print('\nValidation set Mean Squared Error: {}'.format(mse_valid))

# step 3: plot the training set using weights from gradient descent
function = np.poly1d(np.flip(W[:, 0], 0)) 

x_axis = np.linspace(min(X_train), max(X_train))
plt.plot(x_axis, function(X_train), 'r', label = '15 Degree Polynomial Fit') 
plt.scatter(X_train, Y_train, s=10, label='Training set')
plt.xlabel("X")
plt.ylabel("f(x) for dataset and model")
plt.title("Training Set and Model (Gradient descent solution)")
plt.show()

# plot the validation set using weights from gradient descent
function = np.poly1d(np.flip(W[:, 0], 0)) 

x_axis = np.linspace(min(X_valid), max(X_valid))
plt.plot(x_axis, function(X_valid), 'r', label = '15 Degree Polynomial Fit') 
plt.scatter(X_valid, Y_valid, s=10, label='validation set')
plt.xlabel("X")
plt.ylabel("f(x) for dataset and model")
plt.title("Training Set and Model (Gradient descent solution)")
plt.show()

'''
As you can see, this new model generalizes better to our validation set. However, while it does well, we don't really know if this is the absolute most-generalizable model we can create, since we chose 
the lambda_value pretty arbitrarily. We need to tune lambda (aka how much we penalize large weights) to decrease overfitting as much as possible.

This is where our validation set comes in. We already know that our training algorithm works, but we need to tune lambda via trial-and-error to optimize for a model that performs the best on unseen data. 
We choose to use a validation set instead of our test set because we need completely new test set data to obtain a truly fair performance metric at the end. To reiterate, hyperparmaeter-tuning with the 
validation set means that our model is "fit" (to some extent) to the cross validation data, so measuring performance on the validation set gives our model an unfair advantage.
'''
# Step 4: tune lambda 
learning_rate = 0.55
epsilon = 0.0003

# Store results
cross_validation_weights = [] 
cross_validation_MSEs = []
lambda_array = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]

for lambda_value in lambda_array:
  W = np.zeros((16,1))
  W = train_regularized_polynomial_regression(X_train_poly, Y_train, W, learning_rate, epsilon, lambda_value, verbose = False)

  #calculated MSE
  y_valid_pred = np.matmul(X_valid_poly, W)
  mse_valid = np.square(np.subtract(y_valid_pred, Y_valid)).mean()
  print('\nValidation set MSE for {0} lambda: {1}\n'.format(mse_valid, lambda_value))

  cross_validation_weights.append(W)
  cross_validation_MSEs.append(mse_valid)

# Find the optomized lambda value 

lambda_value = lambda_array[np.argmin(cross_validation_MSEs)]
print('Best lambda: {}'.format(lambda_value))

# pick best weight
W = cross_validation_weights[np.argmin(cross_validation_MSEs)]

# calculate MSE on test set
y_test_pred = np.matmul(X_test_poly, W)
mse_test = np.power((y_test_pred - Y_test), 2).mean()
print('\nTest set MSE: {}\n'.format(mse_test))

# plot the results
function = np.poly1d(np.flip(W[:, 0], 0)) 
x_axis = np.linspace(min(X_test), max(X_test))
plt.plot(x_axis, function(x_axis), 'r', label="15 Degree Polynomial Fit")
plt.scatter(X_test, Y_test, s=10, label='Validation set')
plt.xlabel("input x")
plt.ylabel("f(x) for dataset and model")
plt.title("Test Set and Model")
plt.show()